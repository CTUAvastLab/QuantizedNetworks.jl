<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Layers · QuantizedNetworks</title><meta name="title" content="Layers · QuantizedNetworks"/><meta property="og:title" content="Layers · QuantizedNetworks"/><meta property="twitter:title" content="Layers · QuantizedNetworks"/><meta name="description" content="Documentation for QuantizedNetworks."/><meta property="og:description" content="Documentation for QuantizedNetworks."/><meta property="twitter:description" content="Documentation for QuantizedNetworks."/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../../">QuantizedNetworks</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><input class="collapse-toggle" id="menuitem-2" type="checkbox"/><label class="tocitem" for="menuitem-2"><span class="docs-label">Examples</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../examples/mnist/">MNIST Example</a></li><li><a class="tocitem" href="../../examples/flower/">Flower Example</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-3" type="checkbox" checked/><label class="tocitem" for="menuitem-3"><span class="docs-label">Api</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../utilities/">Utilities</a></li><li><a class="tocitem" href="../estimators/">Estimators</a></li><li><a class="tocitem" href="../quantizers/">Quantizers</a></li><li class="is-active"><a class="tocitem" href>Layers</a><ul class="internal"><li><a class="tocitem" href="#QuantDense"><span>QuantDense</span></a></li><li><a class="tocitem" href="#FeatureQuantizer"><span>FeatureQuantizer</span></a></li></ul></li><li><a class="tocitem" href="../blocks/">Blocks</a></li><li><a class="tocitem" href="../l0gate/">L0 gate</a></li><li><a class="tocitem" href="../optimizers/">Optimizers</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Api</a></li><li class="is-active"><a href>Layers</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Layers</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/CTUAvastLab/QuantizedNetworks.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/CTUAvastLab/QuantizedNetworks.jl/blob/main/docs/src/api/layers.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Layers"><a class="docs-heading-anchor" href="#Layers">Layers</a><a id="Layers-1"></a><a class="docs-heading-anchor-permalink" href="#Layers" title="Permalink"></a></h1><h2 id="QuantDense"><a class="docs-heading-anchor" href="#QuantDense">QuantDense</a><a id="QuantDense-1"></a><a class="docs-heading-anchor-permalink" href="#QuantDense" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="QuantizedNetworks.QuantDense" href="#QuantizedNetworks.QuantDense"><code>QuantizedNetworks.QuantDense</code></a> — <span class="docstring-category">Type</span></header><section><div><p>The <code>QuantDense</code> module defines a custom dense layer for neural networks that combines quantization and sparsity techniques for efficient inference.  This module includes various constructors and methods to create and utilize quantized dense layers.</p><p><strong>Constructor</strong></p><ul><li>weight (learnable weights used for the dense layer)</li><li>bias (learnable biases used for the dense layer)</li><li>σ (activation function applied to the layer&#39;s output)</li><li>weight_quantizer (weight quantization function)</li><li>weight_sparsifier (weight sparsification function)</li><li>output_quantizer (output quantization function)</li><li>batchnorm (optional batch normalization layer)</li></ul><p><strong>Functor</strong></p><p><code>QuantDense</code> serves as a functor and it applies the  layer to the input data x. It performs quantization of weights, sparsification, batch normalization (if enabled), and output quantization. If necessary the function resahpes the input.</p><pre><code class="language-julia hljs">using Random; Random.seed!(3);
x = Float32.([1 2]);
kwargs = (;
    init = (dims...) -&gt; ClippedArray(dims...; lo = -1, hi = 1),
    output_quantizer = Ternary(1),
    weight_quantizer = Sign(),
    weight_sparsifier = identity,
    batchnorm = true,
)

qd = QuantDense(1 =&gt; 2, identity; kwargs...)

qd(x)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/CTUAvastLab/QuantizedNetworks.jl/blob/e0cf11600bdbae2e129e9e2ffdc46c198130f917/src/layers/quantdense.jl#L1-L36">source</a></section></article><h3 id="Logic"><a class="docs-heading-anchor" href="#Logic">Logic</a><a id="Logic-1"></a><a class="docs-heading-anchor-permalink" href="#Logic" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="QuantizedNetworks.nn2logic-Tuple{QuantDense}" href="#QuantizedNetworks.nn2logic-Tuple{QuantDense}"><code>QuantizedNetworks.nn2logic</code></a> — <span class="docstring-category">Method</span></header><section><div><p>This function converts a QuantDense layer to a logic-compatible layer by applying the weight quantization and sparsification techniques. It returns a Dense layer suitable for efficient inference.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/CTUAvastLab/QuantizedNetworks.jl/blob/e0cf11600bdbae2e129e9e2ffdc46c198130f917/src/layers/quantdense.jl#L131-L134">source</a></section></article><h3 id="Examples"><a class="docs-heading-anchor" href="#Examples">Examples</a><a id="Examples-1"></a><a class="docs-heading-anchor-permalink" href="#Examples" title="Permalink"></a></h3><pre><code class="language-julia-repl hljs">julia&gt; using Random, QuantizedNetworks; Random.seed!(3);

julia&gt; x = rand(Float32, 1, 2);

julia&gt; qd = QuantDense(1 =&gt; 2)
QuantDense(1 =&gt; 2, identity; weight_lims=(-1.0f0, 1.0f0), bias=false, Ternary(0.05, STE(2)), Sign(STE(2)))

julia&gt; qd(x)
2×2 Matrix{Float32}:
 -1.0  -1.0
  1.0   1.0

julia&gt; d = QuantizedNetworks.nn2logic(qd)
Dense(1 =&gt; 2, Sign(STE(2)))  # 4 parameters

julia&gt; d([3])
2-element Vector{Float32}:
 -1.0
  1.0</code></pre><h2 id="FeatureQuantizer"><a class="docs-heading-anchor" href="#FeatureQuantizer">FeatureQuantizer</a><a id="FeatureQuantizer-1"></a><a class="docs-heading-anchor-permalink" href="#FeatureQuantizer" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="QuantizedNetworks.FeatureQuantizer" href="#QuantizedNetworks.FeatureQuantizer"><code>QuantizedNetworks.FeatureQuantizer</code></a> — <span class="docstring-category">Type</span></header><section><div><p>The <code>FeatureQuantizer</code> module defines a struct and associated functions for feature quantization within a neural network.  Feature quantization involves discretizing input features using learnable weights and biases, and applying an output quantization function.</p><pre><code class="language-julia hljs">function (q::FeatureQuantizer)(x)</code></pre><p><code>FeatureQuantizer</code> serves as a functor and it applies the feature quantization operation to the input data x  using the weights and biases stored in the FeatureQuantizer object and returns the quantized output.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/CTUAvastLab/QuantizedNetworks.jl/blob/e0cf11600bdbae2e129e9e2ffdc46c198130f917/src/layers/featurequantizer.jl#L1-L11">source</a></section></article><h3 id="Forward-pass"><a class="docs-heading-anchor" href="#Forward-pass">Forward pass</a><a id="Forward-pass-1"></a><a class="docs-heading-anchor-permalink" href="#Forward-pass" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="QuantizedNetworks._forward_pass-Tuple{Any, Any, Any}" href="#QuantizedNetworks._forward_pass-Tuple{Any, Any, Any}"><code>QuantizedNetworks._forward_pass</code></a> — <span class="docstring-category">Method</span></header><section><div><p>This is an internal function and performs the forward pass of the feature quantization layer for input data with multiple dimensions. It computes the weighted sum of input data, applies biases, and quantizes the result.  There is also a version for supplying one-dimensional input vector.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/CTUAvastLab/QuantizedNetworks.jl/blob/e0cf11600bdbae2e129e9e2ffdc46c198130f917/src/layers/featurequantizer.jl#L55-L59">source</a></section></article><h3 id="Backpropagation"><a class="docs-heading-anchor" href="#Backpropagation">Backpropagation</a><a id="Backpropagation-1"></a><a class="docs-heading-anchor-permalink" href="#Backpropagation" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ChainRulesCore.rrule-Tuple{typeof(QuantizedNetworks._forward_pass), Any, Any, Any}" href="#ChainRulesCore.rrule-Tuple{typeof(QuantizedNetworks._forward_pass), Any, Any, Any}"><code>ChainRulesCore.rrule</code></a> — <span class="docstring-category">Method</span></header><section><div><p>This function defines the reverse-mode automatic differentiation (AD) rule for the <code>_forward_pass</code> function. It specifies how gradients are propagated backward through the quantization layer during backpropagation.</p><ul><li><code>project_w</code>: A function to project the gradient with respect to weights.</li><li><code>project_b</code>: A function to project the gradient with respect to biases.</li><li><code>project_x</code>: A function to project the gradient with respect to input data.</li></ul><p>This function returns a tuple containing gradients with respect to weights, biases, and input data, along with the projected gradients for each.</p><p>This internal function is used for reverse-mode AD during backpropagation.  It computes gradients for the feature quantization layer and returns the gradients for weights, biases, and input data. Δy is the gradient with respect to the output.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/CTUAvastLab/QuantizedNetworks.jl/blob/e0cf11600bdbae2e129e9e2ffdc46c198130f917/src/layers/featurequantizer.jl#L76-L89">source</a></section></article><h3 id="Examples-2"><a class="docs-heading-anchor" href="#Examples-2">Examples</a><a class="docs-heading-anchor-permalink" href="#Examples-2" title="Permalink"></a></h3><pre><code class="language-julia-repl hljs">julia&gt; using Random, QuantizedNetworks; Random.seed!(3);

julia&gt; x = Float32.([1 2 ;3 4]);

julia&gt; fq = FeatureQuantizer(2,2);

julia&gt; fq(x)
4×2 Matrix{Float32}:
  1.0   1.0
  1.0  -1.0
 -1.0  -1.0
  1.0   1.0</code></pre></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../quantizers/">« Quantizers</a><a class="docs-footer-nextpage" href="../blocks/">Blocks »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="auto">Automatic (OS)</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.1.2 on <span class="colophon-date" title="Monday 6 November 2023 18:19">Monday 6 November 2023</span>. Using Julia version 1.9.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
