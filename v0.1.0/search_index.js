var documenterSearchIndex = {"docs":
[{"location":"examples/mnist/#MNIST-Example","page":"MNIST Example","title":"MNIST Example","text":"","category":"section"},{"location":"examples/mnist/","page":"MNIST Example","title":"MNIST Example","text":"The MNIST dataset is a widely recognized collection of handwritten digits used for training and benchmarking machine learning models.  Following example demonstrates a use case of QuantizedNeteorks package and discrete neural networks on a problem of classifying handwritten digits.","category":"page"},{"location":"examples/mnist/#Create-a-project","page":"MNIST Example","title":"Create a project","text":"","category":"section"},{"location":"examples/mnist/","page":"MNIST Example","title":"MNIST Example","text":"Create a Julia project and add following dependecies","category":"page"},{"location":"examples/mnist/","page":"MNIST Example","title":"MNIST Example","text":"MLDatasets \nQuantizedNetworks \nPlots \nNuLog \nProgressMeter \nStatsBase ","category":"page"},{"location":"examples/mnist/#Utilities","page":"MNIST Example","title":"Utilities","text":"","category":"section"},{"location":"examples/mnist/","page":"MNIST Example","title":"MNIST Example","text":"As it is needed to process and prepare data to be used in traing a network, it is best to create a file for all the helper functions called utilities.jl","category":"page"},{"location":"examples/mnist/#Packages","page":"MNIST Example","title":"Packages","text":"","category":"section"},{"location":"examples/mnist/","page":"MNIST Example","title":"MNIST Example","text":"After creating the file include the following packages and their respective functions to the file.","category":"page"},{"location":"examples/mnist/","page":"MNIST Example","title":"MNIST Example","text":"using QuantizedNetworks\nusing QuantizedNetworks.Flux\nusing MLDatasets\nusing ProgressMeter\n\nusing QuantizedNetworks.Flux.Data: DataLoader\nusing QuantizedNetworks.Flux: onehotbatch, onecold\nusing QuantizedNetworks.Flux.Losses: logitcrossentropy, mse\nusing QuantizedNetworks.Flux.Optimise: update!","category":"page"},{"location":"examples/mnist/#Streamlining-dependecies","page":"MNIST Example","title":"Streamlining dependecies","text":"","category":"section"},{"location":"examples/mnist/","page":"MNIST Example","title":"MNIST Example","text":"In this case it is best to set the environment variable DATADEPS_ALWAYS_ACCEPT to true in order to streamline and automate the management of data dependencies during the execution of a Julia script or program. By doing this, it bypasses any prompts or user interactions that might otherwise occur when data dependencies need to be fetched or updated.","category":"page"},{"location":"examples/mnist/","page":"MNIST Example","title":"MNIST Example","text":"ENV[\"DATADEPS_ALWAYS_ACCEPT\"] = \"true\"","category":"page"},{"location":"examples/mnist/","page":"MNIST Example","title":"MNIST Example","text":"note: Note\nThis practice is useful in automated scripts or when you want to ensure that your Julia program runs without interruption. However, it is important to exercise caution and only use this approach when you are confident about the data dependencies your code relies on, as it bypasses potential prompts that could help prevent unintended changes to your data.","category":"page"},{"location":"examples/mnist/#Data-loading","page":"MNIST Example","title":"Data loading","text":"","category":"section"},{"location":"examples/mnist/","page":"MNIST Example","title":"MNIST Example","text":"Create a function to work with MNIST dataset and a desired batchsize, in order to preprocess the data and prepare it for training.","category":"page"},{"location":"examples/mnist/","page":"MNIST Example","title":"MNIST Example","text":"function createloader(dataset = MLDatasets.MNIST; batchsize::Int = 256)\n    xtrain, ytrain = dataset(:train)[:]\n    train_loader = DataLoader(\n        (Flux.flatten(xtrain), onehotbatch(ytrain, 0:9));\n        batchsize,\n        shuffle=true,\n    )\n\n    xtest, ytest = dataset(:test)[:]\n    test_loader = DataLoader(\n        (Flux.flatten(xtest), onehotbatch(ytest, 0:9));\n        batchsize,\n    )\n    return train_loader, test_loader\nend","category":"page"},{"location":"examples/mnist/","page":"MNIST Example","title":"MNIST Example","text":"createloader (generic function with 2 methods)","category":"page"},{"location":"examples/mnist/#Accuracy-measure","page":"MNIST Example","title":"Accuracy measure","text":"","category":"section"},{"location":"examples/mnist/","page":"MNIST Example","title":"MNIST Example","text":"Add a function to calculate the accuracy of model predictions versus the data.","category":"page"},{"location":"examples/mnist/","page":"MNIST Example","title":"MNIST Example","text":"function accuracy(data_loader, model)\n    acc = 0\n    num = 0\n    for (x, y) in data_loader\n        acc += sum(onecold(model(x)) .== onecold(y))\n        num += size(x)[end]\n    end\n    return acc / num\nend","category":"page"},{"location":"examples/mnist/","page":"MNIST Example","title":"MNIST Example","text":"accuracy (generic function with 1 method)","category":"page"},{"location":"examples/mnist/#Training","page":"MNIST Example","title":"Training","text":"","category":"section"},{"location":"examples/mnist/","page":"MNIST Example","title":"MNIST Example","text":"Add a function to simplify the training of a model.","category":"page"},{"location":"examples/mnist/","page":"MNIST Example","title":"MNIST Example","text":"function train_model(model, opt, train, test; loss = logitcrossentropy, epochs::Int = 30)\n    p = Progress(epochs, 1)\n    ps = Flux.params(model)\n    history = (\n        train_acc = [accuracy(train, model)],\n        test_acc = [accuracy(test, model)],\n    )\n\n    for _ in 1:epochs\n        for (x, y) in train\n            gs = gradient(() -> loss(model(x), y), ps)\n            update!(opt, ps, gs)\n        end\n\n        # compute accuracy\n        push!(history.train_acc, accuracy(train, model))\n        push!(history.test_acc, accuracy(test, model))\n\n        # print progress\n        showvalues = [\n            (:acc_train_0, round(100 * history.train_acc[1]; digits = 2)),\n            (:acc_train, round(100 * history.train_acc[end]; digits = 2)),\n            (:acc_test_0, round(100 * history.test_acc[1]; digits = 2)),\n            (:acc_test, round(100 * history.test_acc[end]; digits = 2)),\n        ]\n        ProgressMeter.next!(p; showvalues)\n    end\n    return history\nend","category":"page"},{"location":"examples/mnist/","page":"MNIST Example","title":"MNIST Example","text":"train_model (generic function with 1 method)","category":"page"},{"location":"examples/mnist/#Create-a-model","page":"MNIST Example","title":"Create a model","text":"","category":"section"},{"location":"examples/mnist/#Dependecies","page":"MNIST Example","title":"Dependecies","text":"","category":"section"},{"location":"examples/mnist/","page":"MNIST Example","title":"MNIST Example","text":"Now create a julia script that will perform the training of neural networks.","category":"page"},{"location":"examples/mnist/","page":"MNIST Example","title":"MNIST Example","text":"Start by adding the required packages, activating the current project and including the utilities.jl file.","category":"page"},{"location":"examples/mnist/","page":"MNIST Example","title":"MNIST Example","text":"using Pkg\nPkg.activate(@__DIR__)\n\nusing Revise\nusing QuantizedNetworks\nusing Plots\nusing Random\n\ninclude(joinpath(@__DIR__, \"utilities.jl\"))","category":"page"},{"location":"examples/mnist/","page":"MNIST Example","title":"MNIST Example","text":"note: Note\nIt is necessary to activate the project yo set the active project environment to be the one located in the directory where the script or notebook is executed, not the global environment.","category":"page"},{"location":"examples/mnist/#Load-data","page":"MNIST Example","title":"Load data","text":"","category":"section"},{"location":"examples/mnist/","page":"MNIST Example","title":"MNIST Example","text":"Set the seed for generating random numers (in order to be able to replicate the results) and call the createloader function to load the training and testing datasets. It is also usefull to prepare the dimensions of the architecture of a neural network with 3 layers (input, hidden and output).","category":"page"},{"location":"examples/mnist/","page":"MNIST Example","title":"MNIST Example","text":"Random.seed!(1234)\ndataset = MNIST\ntrain, test = createloader(dataset; batchsize = 256)\n\ninput_size = size(first(train)[1], 1)\nnclasses = size(first(train)[2], 1)\nnhidden = 256","category":"page"},{"location":"examples/mnist/#Standard-model","page":"MNIST Example","title":"Standard model","text":"","category":"section"},{"location":"examples/mnist/","page":"MNIST Example","title":"MNIST Example","text":"Create a standard Flux.jl model to have as a reference to compare the discrete version performance to.","category":"page"},{"location":"examples/mnist/","page":"MNIST Example","title":"MNIST Example","text":"model = Chain(\n    Dense(input_size => nhidden, relu),\n    Dense(nhidden => nclasses),\n)","category":"page"},{"location":"examples/mnist/","page":"MNIST Example","title":"MNIST Example","text":"Chain(\n  Dense(784 => 256, relu),              # 200_960 parameters\n  Dense(256 => 10),                     # 2_570 parameters\n)                   # Total: 4 arrays, 203_530 parameters, 795.289 KiB.","category":"page"},{"location":"examples/mnist/#Binary-model","page":"MNIST Example","title":"Binary model","text":"","category":"section"},{"location":"examples/mnist/","page":"MNIST Example","title":"MNIST Example","text":"Use the hard hyperbolic tangent function as the activation function. As there are a lot of keyword arguments it is best to create a separate NamedTuple to make it easier to read and understand.","category":"page"},{"location":"examples/mnist/#Hyperparameters","page":"MNIST Example","title":"Hyperparameters","text":"","category":"section"},{"location":"examples/mnist/","page":"MNIST Example","title":"MNIST Example","text":"σ = hardtanh\nkwargs = (;\n    init = (dims...) -> ClippedArray(dims...; lo = -1, hi = 1),\n    output_quantizer = Sign(),\n    batchnorm = true,\n)","category":"page"},{"location":"examples/mnist/","page":"MNIST Example","title":"MNIST Example","text":"(init = var\"#15#16\"(), output_quantizer = Sign(STE(2)), batchnorm = true)","category":"page"},{"location":"examples/mnist/","page":"MNIST Example","title":"MNIST Example","text":"Explanation:","category":"page"},{"location":"examples/mnist/","page":"MNIST Example","title":"MNIST Example","text":"init: It takes a function to initialise the weight matrix, in this case it is an anonymus function that takes n dimensions and creates an n-dimensional ClippedArray which is clamped in the range [-1, 1]\nFor other arguments look up QuantDense","category":"page"},{"location":"examples/mnist/#Model","page":"MNIST Example","title":"Model","text":"","category":"section"},{"location":"examples/mnist/","page":"MNIST Example","title":"MNIST Example","text":"Now create a binary model. It will have two QuantDense layers, but it will be preceded by a layer that is defined as a anonymus function that will only transform the input values to binary values -1 1. In other words it will qunatize the input features of data.","category":"page"},{"location":"examples/mnist/","page":"MNIST Example","title":"MNIST Example","text":"model_bin = Chain(\n    x -> Float32.(ifelse.(x .> 0, 1, -1)),\n    QuantDense(input_size => nhidden, σ; kwargs...),\n    QuantDense(nhidden => nclasses; kwargs...),\n)","category":"page"},{"location":"examples/mnist/","page":"MNIST Example","title":"MNIST Example","text":"Chain(\n  var\"#17#18\"(),\n  QuantDense(\n    Float32[-0.028019346 -0.047897033 … -0.06890707 0.05361874; -0.0464801 -0.02306688 … -0.02808203 -0.0005326818; … ; -0.06333841 0.0009912428 … -0.0015381856 -0.06842575; -0.036003914 -0.03693979 … -0.03655994 0.057226446],  # 200_704 parameters\n    false,\n    identity,\n    Ternary(0.05, STE(2)),\n    identity,\n    Sign(STE(2)),\n    BatchNorm(256, hardtanh),           # 512 parameters, plus 512\n  ),\n  QuantDense(\n    Float32[0.13371286 -0.13981822 … -0.13812053 -0.11148539; 0.12859496 -0.030631518 … -0.045713615 0.1391876; … ; -0.06140963 0.1481502 … -0.001152252 -0.046711177; -0.128069 -0.11804912 … 0.08063337 -0.14401688],  # 2_560 parameters\n    false,\n    identity,\n    Ternary(0.05, STE(2)),\n    identity,\n    Sign(STE(2)),\n    BatchNorm(10),                      # 20 parameters, plus 20\n  ),\n)         # Total: 6 trainable arrays, 203_796 parameters,\n          # plus 4 non-trainable, 532 parameters, summarysize 798.969 KiB.","category":"page"},{"location":"examples/mnist/#Train-a-model","page":"MNIST Example","title":"Train a model","text":"","category":"section"},{"location":"examples/mnist/","page":"MNIST Example","title":"MNIST Example","text":"Define a desired number of epochs, the loss function and run the training of two models, by calling the train_model. function from utilities.jl It will take a few minutes to complete.","category":"page"},{"location":"examples/mnist/","page":"MNIST Example","title":"MNIST Example","text":"epochs = 15\nloss = logitcrossentropy\n\nhistory = train_model(model, AdaBelief(), train, test; epochs, loss)\nhistory_bin = train_model(model_bin, AdaBelief(), train, test; epochs, loss)","category":"page"},{"location":"examples/mnist/","page":"MNIST Example","title":"MNIST Example","text":"(train_acc = [0.10143333333333333, 0.6454, 0.8493, 0.91595, 0.92225, 0.9266166666666666, 0.9364166666666667, 0.9343, 0.9495, 0.94765, 0.9584666666666667, 0.9565333333333333, 0.959, 0.9600666666666666, 0.95585, 0.9612833333333334], test_acc = [0.1026, 0.6464, 0.8468, 0.9075, 0.9137, 0.9159, 0.9288, 0.9228, 0.9336, 0.9336, 0.944, 0.9423, 0.9405, 0.9376, 0.9358, 0.9456])","category":"page"},{"location":"examples/mnist/#Plot-the-results","page":"MNIST Example","title":"Plot the results","text":"","category":"section"},{"location":"examples/mnist/","page":"MNIST Example","title":"MNIST Example","text":"Plot the results to compare the two models and make further adjustments to the hyperparametars.","category":"page"},{"location":"examples/mnist/","page":"MNIST Example","title":"MNIST Example","text":"plt1 = plot(history.train_acc; label = \"normal model\", title = \"Train $(dataset)\");\nplot!(plt1, history_bin.train_acc; label = \"binary model\");\n\nplt2 = plot(history.test_acc; label = \"normal model\", title = \"Test $(dataset)\");\nplot!(plt2, history_bin.test_acc; label = \"binary model\");\n\nplt = plot(\n    plt1,\n    plt2;\n    layout = (2, 1),\n    size = (600, 800),\n    ylims = (0, 1),\n    xlabel = \"epoch\",\n    ylabel = \"accuracy (%)\",\n    legend = :bottomright\n)\n\nsavefig(plt, \"$(dataset).png\")","category":"page"},{"location":"api/estimators/#Estimators","page":"Estimators","title":"Estimators","text":"","category":"section"},{"location":"api/estimators/","page":"Estimators","title":"Estimators","text":"QuantizedNetworks.AbstractEstimator","category":"page"},{"location":"api/estimators/#QuantizedNetworks.AbstractEstimator","page":"Estimators","title":"QuantizedNetworks.AbstractEstimator","text":"AbstractEstimator\n\nEstimators are used for estimation of the gradient of quantizers for which the true gradient does not exist.\n\nEstimators are used for dispatch on backward pass, i.e. for each quantizer (for example Sign)  and it is necessary to define specific method for pullback function.\n\n\n\n\n\n","category":"type"},{"location":"api/estimators/#Straight-through-estimators","page":"Estimators","title":"Straight through estimators","text":"","category":"section"},{"location":"api/estimators/","page":"Estimators","title":"Estimators","text":"QuantizedNetworks.STE\nQuantizedNetworks.PolynomialSTE\nQuantizedNetworks.SwishSTE\nQuantizedNetworks.StochasticSTE","category":"page"},{"location":"api/estimators/#QuantizedNetworks.STE","page":"Estimators","title":"QuantizedNetworks.STE","text":"STE(threshold::Real = 2)\n\nIt is the simplest estimator used in all quantizers like Sign, Heaviside or Ternary.\n\nIt requires a real positive number for threshold parameter, in case a negative number is supplied an ArgumentError exception is thrown.\n\nThreshold is used, in the pullback function, to determine the range of values for which the gradient is calculated by an estimation function. Sign\n\n\n\n\n\n","category":"type"},{"location":"api/estimators/#QuantizedNetworks.PolynomialSTE","page":"Estimators","title":"QuantizedNetworks.PolynomialSTE","text":"PolynomialSTE()\n\nCurrently is only supported for binary Sign quantizer. \n\nDoes not require any additional parameters, simply indicates that a polynomial approximation of sign function is used and its respective derivative.\n\n\n\n\n\n","category":"type"},{"location":"api/estimators/#QuantizedNetworks.SwishSTE","page":"Estimators","title":"QuantizedNetworks.SwishSTE","text":"SwishSTE(β:Real = 5)\n\nCurrently is only supported for binary Sign quantizer. \n\nIt requires a real positive number for β parameter, in case a negative number is supplied an ArgumentError exception is thrown.\n\nβ is used as the parameter in the calculation of the swish function and its derivative in the pullback function. Sign\n\n\n\n\n\n","category":"type"},{"location":"api/estimators/#QuantizedNetworks.StochasticSTE","page":"Estimators","title":"QuantizedNetworks.StochasticSTE","text":"StochasticSTE()\n\nTo be defined.\n\n\n\n\n\n","category":"type"},{"location":"api/optimizers/#Optimizers","page":"Optimizers","title":"Optimizers","text":"","category":"section"},{"location":"api/optimizers/#Bop","page":"Optimizers","title":"Bop","text":"","category":"section"},{"location":"api/optimizers/","page":"Optimizers","title":"Optimizers","text":"Bop\nQuantizedNetworks.Flux.Optimise.apply!(b::Bop, x, Δ)","category":"page"},{"location":"api/optimizers/#QuantizedNetworks.Bop","page":"Optimizers","title":"QuantizedNetworks.Bop","text":"Bop{T}\n\nBop is a custom binary optimizer type that implements a variant of the stochastic gradient descent SGD optimizer with a binary threshold for momentum updates, to decide the direction of the updates.  If the momentum exceeds the threshold and has the same sign as the gradient, the update is set to a positive constant (one(x)); otherwise, it is set to a negative constant (-one(x)). Allows you to control the direction of parameter updates based on the momentum history. It is compatible with the Flux.jl machine learning library.\n\nBop(ρ, τ, momentum)\n\nFields\n\nρ (rho): learning rate hyperparameter (default is 1e-4).\nτ (tau): binary threshold hyperparameter for momentum updates (default is 1e-8).\nmomentum: dictionary that stores the momentum for each parameter (default is an empty dictionary).\n\n\n\n\n\n","category":"type"},{"location":"api/optimizers/#Flux.Optimise.apply!-Tuple{Bop, Any, Any}","page":"Optimizers","title":"Flux.Optimise.apply!","text":"Flux.Optimise.apply!(b::Bop, x, Δ)\n\nA custom apply! function, which is required for optimizers in Flux.jl.\n\nx: The parameters (model weights, bias) to be updated.\nΔ: The gradients or updates for the parameters.\n\n\n\n\n\n","category":"method"},{"location":"api/optimizers/#Case-optimizer","page":"Optimizers","title":"Case optimizer","text":"","category":"section"},{"location":"api/optimizers/","page":"Optimizers","title":"Optimizers","text":"CaseOptimizer\nQuantizedNetworks.Flux.Optimise.apply!(o::CaseOptimizer, x, Δ)","category":"page"},{"location":"api/optimizers/#QuantizedNetworks.CaseOptimizer","page":"Optimizers","title":"QuantizedNetworks.CaseOptimizer","text":"CaseOptimizer\n\nA custom optimizer that works with different optimization strategies based on conditions. It selects the appropriate optimizer from a collection of conditions and associated optimizer objects.  If none of the conditions match, it uses a default optimizer.  This flexibility enables you to adapt the optimization strategy during training based on specific conditions or requirements.\n\nFields\n\noptimizers: A collection of condition-to-optimizer mappings. This field stores pairs of conditions and corresponding optimizer objects.\ndefault: The default optimizer, an instance of AdaBelief, to use when no conditions match any of the conditions defined in the optimizers field.\n\n\n\n\n\n","category":"type"},{"location":"api/optimizers/#Flux.Optimise.apply!-Tuple{CaseOptimizer, Any, Any}","page":"Optimizers","title":"Flux.Optimise.apply!","text":"Flux.Optimise.apply!(o::CaseOptimizer, x, Δ)\n\nA custom apply! function for the CaseOptimizer, which is required for optimizers in Flux.jl. Arguments:\n\nx: The parameters (model weights, bias) to be updated.\nΔ: The gradients or updates for the parameters.\n\n\n\n\n\n","category":"method"},{"location":"api/quantizers/#Quantizers","page":"Quantizers","title":"Quantizers","text":"","category":"section"},{"location":"api/quantizers/","page":"Quantizers","title":"Quantizers","text":"QuantizedNetworks.AbstractQuantizer\nQuantizedNetworks.forward_pass(q::AbstractQuantizer, x)\nQuantizedNetworks.pullback(q::AbstractQuantizer, x)","category":"page"},{"location":"api/quantizers/#QuantizedNetworks.AbstractQuantizer","page":"Quantizers","title":"QuantizedNetworks.AbstractQuantizer","text":"Quantizers are used to limit the range of possible numerical values.  Useful for quantizing neural networks, to work on hardware with limited computational resources.\n\nQuantizer type objects are also functors, i.e. can be called as a function directly supplying the input data and it is the equivalent of calling forward_pass.\n\n\n\n\n\n","category":"type"},{"location":"api/quantizers/#QuantizedNetworks.forward_pass-Tuple{AbstractQuantizer, Any}","page":"Quantizers","title":"QuantizedNetworks.forward_pass","text":"forward_pass(q::AbstractQuantizer, x)\n\nApplies quantizer to Array type x, so each value of x will be quantized.\n\n\n\n\n\n","category":"method"},{"location":"api/quantizers/#QuantizedNetworks.pullback-Tuple{AbstractQuantizer, Any}","page":"Quantizers","title":"QuantizedNetworks.pullback","text":"pullback(q::AbstractQuantizer, x)\n\nReturns gradient of the selected quantizer, with respect to x, by estimating the quantizing function and using the derivative of that estimation to calculate the gradient.\n\n\n\n\n\n","category":"method"},{"location":"api/quantizers/#Binary","page":"Quantizers","title":"Binary","text":"","category":"section"},{"location":"api/quantizers/","page":"Quantizers","title":"Quantizers","text":"QuantizedNetworks.Sign\nQuantizedNetworks.Heaviside","category":"page"},{"location":"api/quantizers/#QuantizedNetworks.Sign","page":"Quantizers","title":"QuantizedNetworks.Sign","text":"Sign(estimator::AbstractEstimator = STE())\n\nDeterministic binary quantizer that returns -1 when the given input is less than zero or Missing and 1 otherwise\n\nsign(x) = begincases\n    -1  x  0 \n    1  x geq 0\nendcases\n\nThe type of the inputs is preserved with exception of Missing input, when it will be quantized into -1.\n\nQuantizers require an estimator to be specified, if none is supplied it will default to Straight Through Estimator STE, with default threshold 2.\n\nEstimators\n\nEstimators are used to estimate the non-existing gradient of the Sign function. They are used only on backward pass.\n\nSTE(threshold::Real = 2): Straight-Through Estimator approximates the sign function using the cliped version of the identity function\n\nclip(x) = begincases\n    -1  x  textthreshold \n    1  x  textthreshold \n    x  textotherwise\nendcases\n\nwith the gradient defined as following \n\nfracpartial clippartial x = begincases\n    1  leftxright leq textthreshold \n    0  leftxright  textthreshold\nendcases\n\nThe following code plots the quantizer function and the first derivative of its linear estimation.      The threshold represents the range of input values for quantization.\n\nusing Plots, QuantizedNetworks: forward_pass, pullback\nq = Sign(STE(1.5))\nx = -5:1/100:5\ny = forward_pass(q, x)\ndy = pullback(q, x)\n\nplot(x,y, label = \"quantizer\", title = \"Sign quantizer - STE (threshold = 1.5)\")\nplot!(x,dy, label=\"gradient\", line = (:path, 2))\n\nPolynomialSTE(): Polynomial estimater approximates the sign function using the piecewise polynomial function\n\npoly(x) = begincases\n    -1  x  -1 \n    2x + x^2  -1 leq x  0 \n    2x - x^2  0 leq x  1 \n    1  textotherwise\nendcases\n\nwith the gradient is defined as\n\nfracpartial polypartial x = begincases\n    2 + 2x  -1 leq x  0 \n    2 - 2x  0 leq x  1 \n    0  textotherwise\nendcases\n\nThe following code plots the quantizer function and the first derivative of its polynomial estimation. \n\nusing QuantizedNetworks: forward_pass, pullback\nq = Sign(PolynomialSTE())\nx = -5:1/100:5\ny = forward_pass(q, x)\ndy = pullback(q, x)\n\nplot(x,y, label = \"quantizer\", title = \"Sign quantizer - PolynomialSTE\")\nplot!(x,dy, label=\"gradient\", line = (:path, 2))\n\nSwishSTE(β=5): SignSwish estimator approximates the sign function using the boundles swish function\n\nsswish_beta(x) = 2sigma(beta x) left(1 + beta x (1 - sigma(beta x))right)\n\nwhere sigma(x) is the sigmoid function and beta  0 controls how fast the function asymptotes to −1 and +1. The gradient is defined as\n\nfracpartial sswish_betapartial x =\nfracbetaleft( 2-beta x tanh left(fracbeta x2right) right)1+cosh (beta x)\n\nThe following code plots the quantizer function and the first derivative of its swish estimation. \n\nusing QuantizedNetworks: forward_pass, pullback\nq = Sign(SwishSTE(2))\nx = -5:1/100:5\ny = forward_pass(q, x)\ndy = pullback(q, x)\n\nplot(x,y, label = \"quantizer\", title = \"Sign quantizer - SwishSTE (β = 2)\")\nplot!(x,dy, label=\"gradient\", line = (:path, 2))\n\nExamples\n\njulia> using QuantizedNetworks: pullback\n\njulia> x = [-2.0, -0.5, 0.0, 0.5, 1.0, missing];\n\njulia> q = Sign()\nSign(STE(2))\n\njulia> q(x)\n6-element Vector{Float64}:\n -1.0\n -1.0\n  1.0\n  1.0\n  1.0\n -1.0\n\njulia> pullback(q, x)\n6-element Vector{Float64}:\n 1.0\n 1.0\n 1.0\n 1.0\n 1.0\n 0.0\n\njulia> pullback(Sign(PolynomialSTE()), x)\n6-element Vector{Float64}:\n 0.0\n 1.0\n 2.0\n 1.0\n 0.0\n 0.0\n\n\n\n\n\n","category":"type"},{"location":"api/quantizers/#QuantizedNetworks.Heaviside","page":"Quantizers","title":"QuantizedNetworks.Heaviside","text":"Heaviside(estimator::AbstractEstimator = STE())\n\nDeterministic binary quantizer that return 0 when the given input is less than zero or Missing and 1 otherwise\n\nheaviside(x) = begincases\n    0  x leq 0 \n    1  x  0\nendcases\n\nThe type of the inputs is preserved with exception of Missing input.\n\nEstimators\n\nEstimators are used to estimate non-existing gradient of the heaviside function. They are used only on backward pass.\n\nSTE(threshold::Real = 2): Straight-Through Estimator approximates the heaviside function using the clip function\n\nclip(x) = begincases\n    0  x  textthreshold \n    1  x  textthreshold \n    x  textotherwise\nendcases\n\nwith the gradient is defined as a clipped identity\n\nfracpartial clippartial x = begincases\n    1  leftxright leq textthreshold \n    0  leftxright  textthreshold\nendcases\n\nThe following code plots the heaviside quantizer function and the first derivative of its linear estimation. \n\nusing QuantizedNetworks: forward_pass, pullback\nq = Heaviside(STE(3))\nx = -5:1/100:5\ny = forward_pass(q, x)\ndy = pullback(q, x)\n\nplot(x,y, label = \"quantizer\", title = \"Heaviside quantizer - STE (threshold = 3)\")\nplot!(x,dy, label=\"gradient\", line = (:path, 2))\n\nExamples\n\njulia> using QuantizedNetworks: pullback\n\njulia> x = [-2.0, -0.5, 0.0, 0.5, 1.0, missing];\n\njulia> q = Heaviside()\nHeaviside(STE(2))\n\njulia> q(x)\n6-element Vector{Float64}:\n 0.0\n 0.0\n 0.0\n 1.0\n 1.0\n 0.0\n\njulia> pullback(q, x)\n6-element Vector{Float64}:\n 1.0\n 1.0\n 1.0\n 1.0\n 1.0\n 0.0\n\n\n\n\n\n","category":"type"},{"location":"api/quantizers/#Ternary","page":"Quantizers","title":"Ternary","text":"","category":"section"},{"location":"api/quantizers/","page":"Quantizers","title":"Quantizers","text":"QuantizedNetworks.Ternary","category":"page"},{"location":"api/quantizers/#QuantizedNetworks.Ternary","page":"Quantizers","title":"QuantizedNetworks.Ternary","text":"Ternary(Δ::T=0.05, estimator::AbstractEstimator = STE())\n\nDeterministic ternary quantizer that return -1 when the given input is less than -Δ, 1 whe the input in greater than Δ, and 0 otherwise. For Missing input, the output is 0.\n\nternary(x) = begincases\n    -1  x  -Delta \n    1  x  Delta \n    0  textotherwise\nendcases\n\nThe type of the inputs is preserved with exception of Missing input.\n\nEstimators\n\nEstimators are used to estimate non-existing gradient of the ternary function. They are used only on backward pass.\n\nSTE(threshold::Real = 2): Straight-Through Estimator approximates the ternary function using the clip function\n\nclip(x) = begincases\n    -1  x  textthreshold \n    1  x  textthreshold \n    x  textotherwise\nendcases\n\nwith the gradient is defined as a clipped identity\n\nfracpartial clippartial x = begincases\n    1  leftxright leq textthreshold \n    -1  leftxright  textthreshold\nendcases\n\nusing QuantizedNetworks: forward_pass, pullback\nq = Ternary(1.5, STE(3))\nx = -5:1/100:5\ny = forward_pass.(q, x)\ndy = pullback.(q, x)\n\nplot(x,y, label = \"quantizer\", title = \"Ternary quantizer - (Δ=1.5, STE threshold=3)\")\nplot!(x,dy, label=\"gradient\", line = (:path, 2))\n\nExamples\n\njulia> using QuantizedNetworks: pullback\n\njulia> x = [-2.0, -0.5, 0.0, 0.5, 1.0, missing];\n\njulia> q = Ternary()\nTernary(0.05, STE(2))\n\njulia> q(x)\n6-element Vector{Float64}:\n -1.0\n -1.0\n  0.0\n  1.0\n  1.0\n  0.0\n\njulia> pullback(q, x)\n6-element Vector{Float64}:\n 1.0\n 1.0\n 1.0\n 1.0\n 1.0\n 0.0\n\n\n\n\n\n","category":"type"},{"location":"api/quantizers/#References","page":"Quantizers","title":"References","text":"","category":"section"},{"location":"api/quantizers/","page":"Quantizers","title":"Quantizers","text":"Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1\nBi-Real Net: Enhancing the Performance of 1-bit CNNs With Improved Representational Capability and Advanced Training Algorithm\nRegularized Binary Network Training","category":"page"},{"location":"api/layers/#Layers","page":"Layers","title":"Layers","text":"","category":"section"},{"location":"api/layers/#QuantDense","page":"Layers","title":"QuantDense","text":"","category":"section"},{"location":"api/layers/","page":"Layers","title":"Layers","text":"QuantDense","category":"page"},{"location":"api/layers/#QuantizedNetworks.QuantDense","page":"Layers","title":"QuantizedNetworks.QuantDense","text":"The QuantDense module defines a custom dense layer for neural networks that combines quantization and sparsity techniques for efficient inference.  This module includes various constructors and methods to create and utilize quantized dense layers.\n\nConstructor\n\nweight (learnable weights used for the dense layer)\nbias (learnable biases used for the dense layer)\nσ (activation function applied to the layer's output)\nweight_quantizer (weight quantization function)\nweight_sparsifier (weight sparsification function)\noutput_quantizer (output quantization function)\nbatchnorm (optional batch normalization layer)\n\nFunctor\n\nQuantDense serves as a functor and it applies the  layer to the input data x. It performs quantization of weights, sparsification, batch normalization (if enabled), and output quantization. If necessary the function resahpes the input.\n\nusing Random; Random.seed!(3);\nx = Float32.([1 2]);\nkwargs = (;\n    init = (dims...) -> ClippedArray(dims...; lo = -1, hi = 1),\n    output_quantizer = Ternary(1),\n    weight_quantizer = Sign(),\n    weight_sparsifier = identity,\n    batchnorm = true,\n)\n\nqd = QuantDense(1 => 2, identity; kwargs...)\n\nqd(x)\n\n\n\n\n\n","category":"type"},{"location":"api/layers/#Logic","page":"Layers","title":"Logic","text":"","category":"section"},{"location":"api/layers/","page":"Layers","title":"Layers","text":"QuantizedNetworks.nn2logic(layer::QuantDense)","category":"page"},{"location":"api/layers/#QuantizedNetworks.nn2logic-Tuple{QuantDense}","page":"Layers","title":"QuantizedNetworks.nn2logic","text":"This function converts a QuantDense layer to a logic-compatible layer by applying the weight quantization and sparsification techniques. It returns a Dense layer suitable for efficient inference.\n\n\n\n\n\n","category":"method"},{"location":"api/layers/#Examples","page":"Layers","title":"Examples","text":"","category":"section"},{"location":"api/layers/","page":"Layers","title":"Layers","text":"julia> using Random, QuantizedNetworks; Random.seed!(3);\n\njulia> x = rand(Float32, 1, 2);\n\njulia> qd = QuantDense(1 => 2)\nQuantDense(1 => 2, identity; weight_lims=(-1.0f0, 1.0f0), bias=false, Ternary(0.05, STE(2)), Sign(STE(2)))\n\njulia> qd(x)\n2×2 Matrix{Float32}:\n -1.0  -1.0\n  1.0   1.0\n\njulia> d = QuantizedNetworks.nn2logic(qd)\nDense(1 => 2, Sign(STE(2)))  # 4 parameters\n\njulia> d([3])\n2-element Vector{Float32}:\n -1.0\n  1.0","category":"page"},{"location":"api/layers/#FeatureQuantizer","page":"Layers","title":"FeatureQuantizer","text":"","category":"section"},{"location":"api/layers/","page":"Layers","title":"Layers","text":"FeatureQuantizer","category":"page"},{"location":"api/layers/#QuantizedNetworks.FeatureQuantizer","page":"Layers","title":"QuantizedNetworks.FeatureQuantizer","text":"The FeatureQuantizer module defines a struct and associated functions for feature quantization within a neural network.  Feature quantization involves discretizing input features using learnable weights and biases, and applying an output quantization function.\n\nfunction (q::FeatureQuantizer)(x)\n\nFeatureQuantizer serves as a functor and it applies the feature quantization operation to the input data x  using the weights and biases stored in the FeatureQuantizer object and returns the quantized output.\n\n\n\n\n\n","category":"type"},{"location":"api/layers/#Forward-pass","page":"Layers","title":"Forward pass","text":"","category":"section"},{"location":"api/layers/","page":"Layers","title":"Layers","text":"QuantizedNetworks._forward_pass(w, b, x)","category":"page"},{"location":"api/layers/#QuantizedNetworks._forward_pass-Tuple{Any, Any, Any}","page":"Layers","title":"QuantizedNetworks._forward_pass","text":"This is an internal function and performs the forward pass of the feature quantization layer for input data with multiple dimensions. It computes the weighted sum of input data, applies biases, and quantizes the result.  There is also a version for supplying one-dimensional input vector.\n\n\n\n\n\n","category":"method"},{"location":"api/layers/#Backpropagation","page":"Layers","title":"Backpropagation","text":"","category":"section"},{"location":"api/layers/","page":"Layers","title":"Layers","text":"QuantizedNetworks.ChainRulesCore.rrule(::typeof(QuantizedNetworks._forward_pass), w, b, x)","category":"page"},{"location":"api/layers/#ChainRulesCore.rrule-Tuple{typeof(QuantizedNetworks._forward_pass), Any, Any, Any}","page":"Layers","title":"ChainRulesCore.rrule","text":"This function defines the reverse-mode automatic differentiation (AD) rule for the _forward_pass function. It specifies how gradients are propagated backward through the quantization layer during backpropagation.\n\nproject_w: A function to project the gradient with respect to weights.\nproject_b: A function to project the gradient with respect to biases.\nproject_x: A function to project the gradient with respect to input data.\n\nThis function returns a tuple containing gradients with respect to weights, biases, and input data, along with the projected gradients for each.\n\nThis internal function is used for reverse-mode AD during backpropagation.  It computes gradients for the feature quantization layer and returns the gradients for weights, biases, and input data. Δy is the gradient with respect to the output.\n\n\n\n\n\n","category":"method"},{"location":"api/layers/#Examples-2","page":"Layers","title":"Examples","text":"","category":"section"},{"location":"api/layers/","page":"Layers","title":"Layers","text":"julia> using Random, QuantizedNetworks; Random.seed!(3);\n\njulia> x = Float32.([1 2 ;3 4]);\n\njulia> fq = FeatureQuantizer(2,2);\n\njulia> fq(x)\n4×2 Matrix{Float32}:\n  1.0   1.0\n  1.0  -1.0\n -1.0  -1.0\n  1.0   1.0","category":"page"},{"location":"api/blocks/#Blocks","page":"Blocks","title":"Blocks","text":"","category":"section"},{"location":"api/blocks/#DenseBlock","page":"Blocks","title":"DenseBlock","text":"","category":"section"},{"location":"api/blocks/","page":"Blocks","title":"Blocks","text":"DenseBlock","category":"page"},{"location":"api/blocks/#QuantizedNetworks.DenseBlock","page":"Blocks","title":"QuantizedNetworks.DenseBlock","text":"The DenseBlock module defines a custom block structure for neural networks. It consists of a chain of layers, making it suitable for creating dense blocks in deep neural networks. Encapsulates functionality for quantized dense layers, batch normalization, and output quantization. It is defined to be a functor object.\n\nConstructor\n\nConstructor creates a DenseBlock object containing a chain of layers. It takes several optional arguments:\n\n(in, out) specifies the input and output dimensions.\nσ is an activation function (default is the identity function).\nweight_quantizer sets the quantizer for weights (default is a ternary quantizer).\noutput_quantizer sets the quantizer for the layer's output (default is a sign quantizer).\nbatchnorm determines whether batch normalization is applied (default is true).\n\nIt constructs a chain of layers including a quantized dense layer, optional batch normalization, and an output quantizer.\n\n\n\n\n\n","category":"type"},{"location":"api/blocks/#Standard-Dense-Layer","page":"Blocks","title":"Standard Dense Layer","text":"","category":"section"},{"location":"api/blocks/","page":"Blocks","title":"Blocks","text":"QuantizedNetworks.Flux.Dense(l::DenseBlock)","category":"page"},{"location":"api/blocks/#Flux.Dense-Tuple{DenseBlock}","page":"Blocks","title":"Flux.Dense","text":"This function is overwritten from the Flux package converts a DenseBlock into a standard dense layer Flux.Dense with quantized weights, adjusted biases, and the specified output quantization.\n\nExtractors serve the purpose of extracting specific components from a DenseBlock like:\n\nextract_dense for quantized dense layer.\nextract_batchnorm for the optional batch normalization layer.\nextract_quantizer for the output quantization function.\n\n\n\n\n\n","category":"method"},{"location":"api/blocks/#Examples","page":"Blocks","title":"Examples","text":"","category":"section"},{"location":"api/blocks/","page":"Blocks","title":"Blocks","text":"julia> using Random, QuantizedNetworks; Random.seed!(3);\n\njulia> db = DenseBlock(2=>2)\nDenseBlock(Chain(QuantizedDense(2 => 2; bias=false, quantizer=Ternary(0.05, STE(2))), BatchNorm(2), Sign(STE(2))))\n\njulia> x = rand(Float32, 2, 4)\n2×4 Matrix{Float32}:\n 0.940675  0.100403   0.789168  0.582228\n 0.999979  0.0921143  0.698426  0.496285\n\njulia> db(x)\n2×4 Matrix{Float32}:\n -1.0   1.0   1.0   1.0\n  1.0  -1.0  -1.0  -1.0","category":"page"},{"location":"api/blocks/#FeatureBlock","page":"Blocks","title":"FeatureBlock","text":"","category":"section"},{"location":"api/blocks/","page":"Blocks","title":"Blocks","text":"FeatureBlock","category":"page"},{"location":"api/blocks/#QuantizedNetworks.FeatureBlock","page":"Blocks","title":"QuantizedNetworks.FeatureBlock","text":"A custom struct representing a feature block.  Holds a collection of layers which include feature quantization layers  and an optional quantizer and where each layer performs a specific operation on the input data. The struct is also a functor, so it can be used as a function.\n\nConstructor\n\nYou specify the dimensionality of the input features dim and the number of quantization levels k.  Additionally, you can choose to include an extra quantizer layer for handling missing data by setting output_missing to true.  The quantizer argument lets you specify the quantization function to be used (with a default of Sign()),  and any additional keyword arguments are passed to the FeatureQuantizer constructor. \n\n\n\n\n\n","category":"type"},{"location":"api/blocks/#Examples-2","page":"Blocks","title":"Examples","text":"","category":"section"},{"location":"api/blocks/","page":"Blocks","title":"Blocks","text":"julia> using Random, QuantizedNetworks; Random.seed!(3);\n\njulia> fb = FeatureBlock(2, 2)\nFeatureBlock(Parallel(vcat, FeatureQuantizer(2 => 4; quantizer=Sign(STE(2)))))\n\njulia> x = rand(Float32, 2, 1)\n2×1 Matrix{Float32}:\n 0.8521847\n 0.7965402\n\njulia> fb(x)\n4×1 Matrix{Float32}:\n  1.0\n  1.0\n  1.0\n -1.0","category":"page"},{"location":"api/utilities/#Utilities","page":"Utilities","title":"Utilities","text":"","category":"section"},{"location":"api/utilities/#ClippedArray","page":"Utilities","title":"ClippedArray","text":"","category":"section"},{"location":"api/utilities/","page":"Utilities","title":"Utilities","text":"QuantizedNetworks.ClippedArray","category":"page"},{"location":"api/utilities/#QuantizedNetworks.ClippedArray","page":"Utilities","title":"QuantizedNetworks.ClippedArray","text":"ClippedArray(x::AbstractArray, lo::Real=-1, hi::Real=1)\nClippedArray(dims...; lo::Real=-1, hi::Real=1, init = glorot_uniform)\n\nRepresents an Array type with elements clipped between lo and hi, if nothing is set range defaults to [-1, +1].  It still works in the case that hi is less then lo. Supports all standard Array type operations and functions, as well there are type aliases for 2D and 3D arrays (i.e. ClippedVector, ClippedMatrix).\n\nExamples\n\njulia> C = ClippedArray([-4.0, -0.8, 0.1, 1.2], -1, 1.2)\n4-element ClippedArray{Float64, 1, Vector{Float64}}:\n -1.0\n -0.8\n  0.1\n  1.2\n\njulia> C[[2, 4]] = [-2.4, 2.6]\n2-element Vector{Float64}:\n -2.4\n  2.6\n\njulia> C\n4-element ClippedArray{Float64, 1, Vector{Float64}}:\n -1.0\n -1.0\n  0.1\n  1.2\n\njulia> using Random; Random.seed!(3);\n\njulia> ClippedArray(2, 3)\n2×3 ClippedArray{Float32, 2, Matrix{Float32}}:\n  0.843751  -1.0  0.96547\n -0.351124   1.0  1.0\n\n\n\n\n\n","category":"type"},{"location":"api/l0gate/#L0-gate","page":"L0 gate","title":"L0 gate","text":"","category":"section"},{"location":"api/l0gate/","page":"L0 gate","title":"L0 gate","text":"L0Gate","category":"page"},{"location":"api/l0gate/#QuantizedNetworks.L0Gate","page":"L0 gate","title":"QuantizedNetworks.L0Gate","text":"struct L0Gate{T, S}\n\nRepresents an L0Gate which applies L0 regularization during neural network training.\n\nL0Gate(logα = 10; β = 2/3, dims = :, active = nothing)\n\nFields\n\nlogα::T: Controls the strength of L0 regularization. (defaults to 10)\nβ::S: Controls \"temperature\" of a sigmoid function. (defaults to 2/3)\ndims::Union{Colon, Int}: Specifies the dimensions to which L0 regularization is applied. (defaults to :)\nactive::RefValue{Union{Bool, Nothing}}: Indicates whether the L0Gate is active (regularization is applied). (defaults to nothing)\n\n\n\n\n\n","category":"type"},{"location":"api/l0gate/#Functions","page":"L0 gate","title":"Functions","text":"","category":"section"},{"location":"api/l0gate/","page":"L0 gate","title":"L0 gate","text":"QuantizedNetworks.isactive(c::L0Gate)\nQuantizedNetworks.Flux.testmode!(c::L0Gate, mode=true)","category":"page"},{"location":"api/l0gate/#QuantizedNetworks.isactive-Tuple{L0Gate}","page":"L0 gate","title":"QuantizedNetworks.isactive","text":"isactive(c::L0Gate)\n\nChecks if the L0Gate is active (applies regularization).\n\n\n\n\n\n","category":"method"},{"location":"api/l0gate/#Flux.testmode!","page":"L0 gate","title":"Flux.testmode!","text":"Flux.testmode!(c::L0Gate, mode=true)\n\nSets the testing mode for the L0Gate object. If mode is true, it sets the active field to nothing, effectively turning off L0 regularization during testing. If mode is false, it sets the active field to true, enabling L0 regularization during testing. If mode is :auto, it toggles the active field.\n\n\n\n\n\n","category":"function"},{"location":"api/l0gate/#Helper-functions","page":"L0 gate","title":"Helper functions","text":"","category":"section"},{"location":"api/l0gate/","page":"L0 gate","title":"L0 gate","text":"QuantizedNetworks._shape(s, ::Colon)\nQuantizedNetworks._shape(s, dims)\nQuantizedNetworks.shift(x::T, lo::Real = -0.1, hi::Real = 1.1) where {T}\nQuantizedNetworks.l0gate_train(x::AbstractArray{T}, logα, β; dims = :) where {T}\nQuantizedNetworks.l0gate_test(::AbstractArray{T}, logα, β) where {T}","category":"page"},{"location":"api/l0gate/#QuantizedNetworks._shape-Tuple{Any, Colon}","page":"L0 gate","title":"QuantizedNetworks._shape","text":"_shape(s, ::Colon)\n\nComputes the size of an array when applying L0 regularization to all dimensions\n\n\n\n\n\n","category":"method"},{"location":"api/l0gate/#QuantizedNetworks._shape-Tuple{Any, Any}","page":"L0 gate","title":"QuantizedNetworks._shape","text":"_shape(s, dims)\n\nComputes the size of an array when applying L0 regularization to specified dimensions.\n\n\n\n\n\n","category":"method"},{"location":"api/l0gate/#QuantizedNetworks.shift-Union{Tuple{T}, Tuple{T, Real}, Tuple{T, Real, Real}} where T","page":"L0 gate","title":"QuantizedNetworks.shift","text":"shift(x::T, lo::Real = -0.1, hi::Real = 1.1) where {T}\n\nShifts the input x to a specified range [lo, hi].\n\n\n\n\n\n","category":"method"},{"location":"api/l0gate/#QuantizedNetworks.l0gate_train-Union{Tuple{T}, Tuple{AbstractArray{T}, Any, Any}} where T","page":"L0 gate","title":"QuantizedNetworks.l0gate_train","text":"l0gate_train(x::AbstractArray{T}, logα, β; dims = :) where {T}\n\nApplies L0 regularization during training.\n\n\n\n\n\n","category":"method"},{"location":"api/l0gate/#QuantizedNetworks.l0gate_test-Union{Tuple{T}, Tuple{AbstractArray{T}, Any, Any}} where T","page":"L0 gate","title":"QuantizedNetworks.l0gate_test","text":"l0gate_test(::AbstractArray{T}, logα, β) where {T}\n\nApplies L0 regularization during testing.\n\n\n\n\n\n","category":"method"},{"location":"examples/flower/#Flower-Example","page":"Flower Example","title":"Flower Example","text":"","category":"section"},{"location":"#QuantizedNetworks.jl","page":"Home","title":"QuantizedNetworks.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation for QuantizedNetworks.jl","category":"page"}]
}
